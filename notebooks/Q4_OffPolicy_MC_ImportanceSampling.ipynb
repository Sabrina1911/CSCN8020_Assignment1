{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6085495",
   "metadata": {},
   "source": [
    "### **Q4 — Off-Policy Monte Carlo**\n",
    "\n",
    "#### **Problem Statement**\n",
    "\n",
    "In this problem, we use the same gridworld environment from Problem 3 and estimate the value function using **off-policy Monte Carlo methods with Importance Sampling**.\n",
    "\n",
    "- The **behavior policy**, b(a|s), is a fixed random policy.\n",
    "- The **target policy**, π(a|s), is a greedy policy with respect to the learned action-value function Q(s,a).\n",
    "- The discount factor is γ = 0.9.\n",
    "\n",
    "Two off-policy Monte Carlo estimators are implemented:\n",
    "\n",
    "1. Ordinary Importance Sampling (OIS)\n",
    "2. Weighted Importance Sampling (WIS)\n",
    "\n",
    "For reference, we also compute the optimal value function using **Value Iteration** and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d234a8",
   "metadata": {},
   "source": [
    "#### **Gridworld Environment**\n",
    "\n",
    "We use a **5 × 5 gridworld**, identical to the environment used in **Problem 3**.\n",
    "\n",
    "Environment details:\n",
    "\n",
    "- Grid size: 5 rows × 5 columns\n",
    "- Terminal (goal) state: bottom-right cell\n",
    "- Blocked (unreachable) states: same positions as in Problem 3\n",
    "- Available actions: Up, Down, Left, Right\n",
    "- Reward: −1 for each step\n",
    "- An episode terminates when the agent reaches the goal state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4193d3",
   "metadata": {},
   "source": [
    "#### **Policies**\n",
    "\n",
    "**Behavior Policy**\n",
    "\n",
    "The behavior policy b(a|s) selects actions uniformly at random from the action set. This policy is fixed and is used only to generate episodes.\n",
    "\n",
    "**Target Policy**\n",
    "\n",
    "The target policy π(a|s) is greedy with respect to the current estimate of Q(s,a). If multiple actions share the same maximum value, one is selected randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc30ded",
   "metadata": {},
   "source": [
    "#### **Episode Generation**\n",
    "\n",
    "Episodes are generated by starting from a random non-terminal state and following the behavior policy b(a|s).\n",
    "\n",
    "Each episode consists of a sequence of:\n",
    "(state, action, reward)\n",
    "\n",
    "until the goal state is reached or a maximum number of steps is exceeded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a0b34",
   "metadata": {},
   "source": [
    "#### **Value Matrix and Greedy Policy Display**\n",
    "\n",
    "At selected episode indices k, we record:\n",
    "\n",
    "- The estimated state-value function V_k, displayed as a **5 × 5 matrix**\n",
    "- The greedy policy derived from Q_k, shown using directional arrows\n",
    "\n",
    "This presentation matches the value tables and policy diagrams shown in the lecture slides for the 5 × 5 gridworld."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f073f4a7",
   "metadata": {},
   "source": [
    "#### **Logger**\n",
    "\n",
    "During training, snapshots of the current estimates are written to log files under:\n",
    "\n",
    "`./logs/q4/`\n",
    "\n",
    "Each snapshot includes:\n",
    "- The episode index k\n",
    "- The current value matrix V_k\n",
    "- The greedy policy corresponding to Q_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e530046",
   "metadata": {},
   "source": [
    "#### **Initialization of Q and Weight Tracking**\n",
    "\n",
    "We maintain the following data structures:\n",
    "\n",
    "- Q(s,a): action-value estimates\n",
    "- C(s,a): cumulative weights for Weighted Importance Sampling\n",
    "- N(s,a): counters used for Ordinary Importance Sampling\n",
    "\n",
    "All values are initialized to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798522fd",
   "metadata": {},
   "source": [
    "#### **Off-policy Monte Carlo Control (Weighted Importance Sampling)**\n",
    "\n",
    "For each episode generated under the behavior policy, returns are computed backward from the end of the episode.\n",
    "\n",
    "Weighted Importance Sampling is used to update Q(s,a), which reduces variance by normalizing the importance weights using cumulative sums.\n",
    "\n",
    "If the behavior action at a state does not match the greedy target action, the update for that episode stops early."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c86910",
   "metadata": {},
   "source": [
    "#### **Off-policy Monte Carlo Control (Ordinary Importance Sampling)**\n",
    "\n",
    "Ordinary Importance Sampling uses the same importance ratios but applies them directly when updating Q(s,a).\n",
    "\n",
    "This method is unbiased but typically has higher variance compared to Weighted Importance Sampling.\n",
    "\n",
    "An incremental averaging form is used to match the pseudocode presented in the lecture notes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1e245",
   "metadata": {},
   "source": [
    "#### **Running Both Algorithms and Writing Logs**\n",
    "\n",
    "Both Ordinary IS and Weighted IS are run for the same number of episodes.\n",
    "\n",
    "Separate log files are created for each method so their convergence behavior and stability can be compared directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe349f4",
   "metadata": {},
   "source": [
    "#### **Value Iteration Baseline**\n",
    "\n",
    "Value Iteration is used to compute the optimal value function for the 5 × 5 gridworld.\n",
    "\n",
    "The resulting value matrix and greedy policy serve as a baseline for evaluating the Monte Carlo estimates obtained using off-policy learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5116405",
   "metadata": {},
   "source": [
    "#### **Importing the Q4 Implementation**\n",
    "\n",
    "In this notebook, we **reuse the Python implementation written for Question 4** (`off_policy_mc.py`) instead of re-implementing all algorithms inside the notebook.\n",
    "\n",
    "This approach keeps the notebook **clean, readable, and consistent** with the actual code used to generate the results.\n",
    "\n",
    "To allow the notebook to access the `src/` folder, we first add the **project root** to Python’s module search path. This makes it possible to directly import the Q4\n",
    "Gridworld environment and Monte Carlo algorithms.\n",
    "\n",
    "### What is being imported\n",
    "\n",
    "- **GridworldConfig**  \n",
    "  Defines the grid size, discount factor (γ), rewards, goal state, and episode settings.\n",
    "\n",
    "- **GridworldMDP**  \n",
    "  Implements the same **5×5 Gridworld** used in Q3 and Q4, including state transitions   and reward-on-arrival logic.\n",
    "\n",
    "- **OffPolicyMCControl**  \n",
    "  Contains the two off-policy Monte Carlo control methods used in Q4:\n",
    "  - Ordinary Importance Sampling (OIS)\n",
    "  - Weighted Importance Sampling (WIS)\n",
    "\n",
    "- **run_q4_variations**  \n",
    "  A helper function that runs:\n",
    "  - Value Iteration (baseline)\n",
    "  - Off-policy Monte Carlo with OIS\n",
    "  - Off-policy Monte Carlo with WIS  \n",
    "\n",
    "  and returns all results in a structured format for comparison and analysis.\n",
    "\n",
    "By separating **implementation (Python file)** from **analysis (notebook)**, the notebook can focus on results, comparison, and interpretation rather than\n",
    "low-level algorithm details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13d85128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Add project root to Python path (so src/ works)\n",
    "PROJECT_ROOT = Path.cwd().parent  # adjust if notebook is deeper\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.q4.off_policy_mc import (\n",
    "    GridworldConfig,\n",
    "    GridworldMDP,\n",
    "    OffPolicyMCControl,\n",
    "    run_q4_variations,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530d6438",
   "metadata": {},
   "source": [
    "#### **Importing the Q4 Implementation**\n",
    "\n",
    "In this notebook, we reuse the Python implementation written for Question 4 (off_policy_mc.py) instead of rewriting all algorithms inside the notebook.\n",
    "\n",
    "This approach keeps the notebook:\n",
    "* clean and easy to read\n",
    "* consistent with the actual code used to generate results\n",
    "* aligned with good software practice (one implementation, reused everywhere)\n",
    "\n",
    "To allow the notebook to access the src/ folder, we first add the project root to Python’s module search path. This makes it possible to directly import the Gridworld environment and Monte Carlo algorithms used in Q4.\n",
    "\n",
    "**What this code does**\n",
    "* Adds the project root directory to sys.path so Python can locate src/\n",
    "* Imports the core Q4 components needed for running experiments and analyzing results\n",
    "\n",
    "**What is being imported**\n",
    "* *GridworldConfig*\n",
    "Defines the grid size, discount factor (γ), rewards, goal state, and episode settings.\n",
    "\n",
    "* *GridworldMDP*\n",
    "Implements the same 5 × 5 Gridworld environment used in Q3 and Q4, including state transitions and reward-on-arrival logic.\n",
    "\n",
    "* *OffPolicyMCControl*\n",
    "Contains the two off-policy Monte Carlo control methods used in Q4:\n",
    "   - Ordinary Importance Sampling (OIS)\n",
    "   - Weighted Importance Sampling (WIS)\n",
    "\n",
    "* *run_q4_variations*\n",
    "A helper function that runs:\n",
    "   - Value Iteration (baseline)\n",
    "   - Off-policy Monte Carlo with OIS\n",
    "   - Off-policy Monte Carlo with WIS\n",
    "\n",
    "and returns all results in a structured format for comparison and analysis.\n",
    "\n",
    "By separating the implementation (Python file) from the analysis (notebook), the notebook can focus on results, comparison, and interpretation rather than low-level algorithm details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcc46fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Add project root to Python path (so src/ works)\n",
    "PROJECT_ROOT = Path.cwd().parent  # adjust if notebook is deeper\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.q4.off_policy_mc import (\n",
    "    GridworldConfig,\n",
    "    GridworldMDP,\n",
    "    OffPolicyMCControl,\n",
    "    run_q4_variations,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e672c",
   "metadata": {},
   "source": [
    "#### **Value Iteration Baseline**\n",
    "\n",
    "Before running the off-policy Monte Carlo methods, we first compute a Value Iteration (VI) baseline for the same 5×5 Gridworld.\n",
    "\n",
    "Value Iteration assumes that the full environment is known (state transitions and rewards). It repeatedly updates all states using the Bellman optimality rule until the values converge. Because the gridworld is small and deterministic, this process converges very quickly.\n",
    "\n",
    "This baseline serves two purposes:\n",
    "* It provides the optimal value function (V*) for the gridworld\n",
    "* It gives us a reference policy to compare against the Monte Carlo results\n",
    "\n",
    "**What this cell outputs**\n",
    "* Number of iterations needed for Value Iteration to converge\n",
    "* Runtime, showing how fast model-based methods can be\n",
    "* The optimal value matrix (V*), displayed in grid form\n",
    "* The greedy optimal policy (π*), shown using arrow symbols\n",
    "\n",
    "**Why this matters for Q4**\n",
    "\n",
    "The off-policy Monte Carlo methods in Q4 do not know the environment model and must learn only from sampled episodes. By comparing their learned values and policies against this Value Iteration baseline, we can clearly see:\n",
    "* how close Monte Carlo learning gets to the optimal solution\n",
    "* the effect of variance in Ordinary vs. Weighted Importance Sampling\n",
    "\n",
    "This makes Value Iteration an essential benchmark, even though Q4 itself focuses on Monte Carlo learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a6b635c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Value Iteration Baseline ===\n",
      "iters: 9 time(s): 0.000413\n",
      " -0.43    0.63    1.81    3.12    4.58*\n",
      "  0.63    1.81    3.12    4.58    6.20 \n",
      "  1.81    3.12    4.58*   6.20    8.00 \n",
      "  3.12*   4.58    6.20    8.00   10.00 \n",
      "  4.58    6.20    8.00   10.00     G   \n",
      "\n",
      "Policy:\n",
      " →   →   →   ↓   ↓ \n",
      " →   →   →   →   ↓ \n",
      " →   ↓   →   →   ↓ \n",
      " →   →   →   →   ↓ \n",
      " →   →   →   →   G \n"
     ]
    }
   ],
   "source": [
    "mdp = GridworldMDP(cfg)\n",
    "\n",
    "V_vi, pi_vi, it_vi, t_vi = mdp.value_iteration()\n",
    "\n",
    "print(\"=== Value Iteration Baseline ===\")\n",
    "print(\"iters:\", it_vi, \"time(s):\", round(t_vi, 6))\n",
    "print(mdp.format_V(V_vi, decimals=2))\n",
    "print(\"\\nPolicy:\")\n",
    "print(mdp.format_pi(pi_vi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e5300",
   "metadata": {},
   "source": [
    "This function executes the entire Q4 workflow using the configuration defined earlier:\n",
    "* Value Iteration (baseline)\n",
    "* Off-policy Monte Carlo using Ordinary Importance Sampling (OIS)\n",
    "* Off-policy Monte Carlo using Weighted Importance Sampling (WIS)\n",
    "\n",
    "The function uses the same grid size, discount factor, episode count, and snapshot schedule defined in GridworldConfig, ensuring consistency across all methods.\n",
    "\n",
    "**What this step does**\n",
    "* Runs Value Iteration to compute the optimal value function and policy\n",
    "* Runs Off-policy Monte Carlo (OIS) using randomly generated episodes\n",
    "* Runs Off-policy Monte Carlo (WIS) using the same episode settings\n",
    "* Writes detailed log files and CSV snapshots to logs/q4/\n",
    "* Collects all results into a single output object for later analysis\n",
    "\n",
    "**Output**\n",
    "\n",
    "The returned object contains:\n",
    "* Value function and policy from Value Iteration\n",
    "* Value functions and policies from OIS and WIS\n",
    "* Runtime information\n",
    "* Paths to the generated log and CSV files\n",
    "\n",
    "This allows the notebook to focus on comparison and interpretation, while the full computation is handled inside the Python implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdc54cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = run_q4_variations(\n",
    "    episodes=cfg.episodes,\n",
    "    gamma=cfg.gamma,\n",
    "    max_steps=cfg.max_steps,\n",
    "    snapshot_episodes=cfg.snapshot_episodes,\n",
    "    log_dir=\"logs/q4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc1e0e1",
   "metadata": {},
   "source": [
    "* **Off-policy Monte Carlo Results (OIS vs WIS)**\n",
    "\n",
    "In this section, we display the final results of the off-policy Monte Carlo control methods used in Question 4 and compare their learned value functions and policies.\n",
    "\n",
    "Both methods learn from sampled episodes, rather than using full knowledge of the environment rules.\n",
    "\n",
    "* **Ordinary Importance Sampling (OIS)**\n",
    "\n",
    "The table below shows the state-value function and greedy policy learned using Ordinary Importance Sampling (OIS).\n",
    "\n",
    "OIS applies raw importance weights when reusing data collected from a random behavior policy.\n",
    "Because these weights can grow very large, the resulting value estimates often show large spikes and instability, even after many episodes.\n",
    "\n",
    "This behavior is expected and highlights the high variance problem of ordinary importance sampling in practice.\n",
    "\n",
    "* **Weighted Importance Sampling (WIS)**\n",
    "\n",
    "The second table shows the state-value function and greedy policy learned using Weighted Importance Sampling (WIS).\n",
    "\n",
    "WIS improves stability by normalizing importance weights, which smooths learning and prevents extreme updates.\n",
    "As a result, the learned values are stable and closely match the Value Iteration baseline from earlier.\n",
    "\n",
    "This demonstrates why WIS is preferred over OIS for off-policy Monte Carlo control in finite-sample settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c773faca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Off-policy MC (Ordinary IS) ===\n",
      " -2.51   -3.44   -2.78   -3.88   -3.05*\n",
      " -3.66   16.77   -2.62   51.10   -2.54 \n",
      " -2.50   18.85   35.74*  48.59   10.30 \n",
      " -3.79*   0.94   13.97   17.15   10.58 \n",
      " -3.47   -3.12   21.08   59.30     G   \n",
      "\n",
      "Policy:\n",
      " ←   ↑   →   ↓   ↓ \n",
      " ↓   ↓   →   ↓   ↓ \n",
      " ↑   ↑   ↓   ↓   ↓ \n",
      " ↑   →   →   →   → \n",
      " ←   →   ↓   ↓   G \n",
      "\n",
      "=== Off-policy MC (Weighted IS) ===\n",
      " -0.44    0.62    1.80    3.10    4.56*\n",
      "  0.62    1.79    3.10    4.56    6.18 \n",
      "  1.80    3.10    4.56*   6.18    7.99 \n",
      "  3.11*   4.56    6.18    7.99   10.00 \n",
      "  4.55    6.18    7.99   10.00     G   \n",
      "\n",
      "Policy:\n",
      " →   ↓   →   ↓   ↓ \n",
      " →   →   →   ↓   ↓ \n",
      " →   ↓   →   ↓   ↓ \n",
      " →   →   ↓   →   ↓ \n",
      " →   →   →   →   G \n"
     ]
    }
   ],
   "source": [
    "print(\"=== Off-policy MC (Ordinary IS) ===\")\n",
    "print(mdp.format_V(out[\"OIS\"][\"V\"], decimals=2))\n",
    "print(\"\\nPolicy:\")\n",
    "print(mdp.format_pi(out[\"OIS\"][\"pi\"]))\n",
    "\n",
    "print(\"\\n=== Off-policy MC (Weighted IS) ===\")\n",
    "print(mdp.format_V(out[\"WIS\"][\"V\"], decimals=2))\n",
    "print(\"\\nPolicy:\")\n",
    "print(mdp.format_pi(out[\"WIS\"][\"pi\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a20344",
   "metadata": {},
   "source": [
    "#### **Evidence: Comparing Weighted MC to Value Iteration**\n",
    "\n",
    "In this cell, we quantitatively compare the final value function learned using Weighted Importance Sampling (WIS) with the optimal value function obtained from Value Iteration (VI).\n",
    "\n",
    "Since Value Iteration computes the optimal solution directly using the known environment rules, it serves as a reference baseline. If the WIS method is working correctly, its learned values should be very close to the VI values, even though WIS learns only from sampled episodes.\n",
    "\n",
    "To measure this closeness, we compute:\n",
    "* *Maximum absolute difference:* max∣VWIS​−VVI​∣, This shows the largest error at any state.\n",
    "* *Mean absolute difference:*    mean(∣VWIS​−VVI​∣), This shows the average error across all states.\n",
    "\n",
    "**Interpretation of the result**\n",
    "* A small maximum difference indicates that WIS has converged very close to the optimal solution.\n",
    "* The low mean difference confirms that this agreement holds across the entire grid, not just a few states.\n",
    "* This provides strong numerical evidence that Weighted Importance Sampling is stable and effective, even without knowing the full MDP model.\n",
    "\n",
    "In contrast, Ordinary Importance Sampling (OIS) does not achieve this level of closeness due to its high variance, which is why WIS is preferred in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62aac6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evidence (WIS vs VI) ===\n",
      "max |V_WIS - V_VI|  = 0.0276\n",
      "mean|V_WIS - V_VI| = 0.0144\n"
     ]
    }
   ],
   "source": [
    "V_wis = out[\"WIS\"][\"V\"]\n",
    "V_vi  = out[\"VI\"][\"V\"]\n",
    "\n",
    "max_diff = float(np.max(np.abs(V_wis - V_vi)))\n",
    "mean_diff = float(np.mean(np.abs(V_wis - V_vi)))\n",
    "\n",
    "print(\"=== Evidence (WIS vs VI) ===\")\n",
    "print(f\"max |V_WIS - V_VI|  = {max_diff:.4f}\")\n",
    "print(f\"mean|V_WIS - V_VI| = {mean_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f37ce0",
   "metadata": {},
   "source": [
    "#### **Comparison Table**\n",
    "\n",
    "| Method                   | How it learns                                     | Needs full model? | Stability   | Speed     | Main takeaway                                     |\n",
    "| ------------------------ | ------------------------------------------------- | ----------------- | ----------- | --------- | ------------------------------------------------- |\n",
    "| **Value Iteration (VI)** | Uses equations and known rules of the environment | Yes               | Very stable | Very fast | Knows the map → walks straight to the best answer |\n",
    "| **Off-policy MC (OIS)**  | Learns from random trial-and-error episodes       | No                | Unstable    | Slow      | Raw weighting causes big value spikes             |\n",
    "| **Off-policy MC (WIS)**  | Learns from episodes but averages carefully       | No                | Stable      | Slow      | Smoothed learning → ends up close to VI           |\n",
    "\n",
    "* Value Iteration (VI): Solves the problem by repeatedly applying equations using full knowledge of the environment.\n",
    "* Ordinary Importance Sampling (OIS): Learns from random experiences but can become unstable due to large correction weights.\n",
    "* Weighted Importance Sampling (WIS): Improves OIS by normalizing weights, resulting in smoother and more reliable learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc95125",
   "metadata": {},
   "source": [
    "#### **Final Comparison**\n",
    "\n",
    "In this question, we solved the same 5×5 gridworld problem using off-policy Monte Carlo learning and compared it against a Value Iteration (VI) baseline. While all methods aim to estimate the optimal value function, they differ significantly in how they learn.\n",
    "\n",
    "Value Iteration uses full knowledge of the environment, including the reward function and transition rules. Because of this, it converges very quickly and produces smooth, stable values across the grid. In our experiment, VI converged in only a few iterations and served as a reliable reference solution.\n",
    "\n",
    "In contrast, Monte Carlo methods do not use the environment model. Instead, they learn purely from sampled episodes generated by a random behavior policy. This makes Monte Carlo more realistic for situations where the model is unknown, but it also introduces higher variability.\n",
    "\n",
    "The Ordinary Importance Sampling (OIS) approach showed clear instability. Since it applies raw importance weights, some state values became extremely large. This behavior is expected and highlights a known weakness of OIS: variance can grow rapidly when the behavior policy differs from the target policy.\n",
    "\n",
    "The Weighted Importance Sampling (WIS) method addressed this issue by normalizing importance weights. As a result, WIS produced a stable value function that closely matched the Value Iteration solution. The maximum difference between WIS and VI was approximately 0.0276, which is very small and provides strong evidence that WIS converges to the correct solution with enough samples.\n",
    "\n",
    "Overall, this experiment demonstrates that while Monte Carlo methods are slower and noisier than Value Iteration, Weighted Importance Sampling provides a reliable and model-free alternative that can closely approximate optimal values when sufficient experience is available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
