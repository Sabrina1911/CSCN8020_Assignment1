{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04de5311",
   "metadata": {},
   "source": [
    "### **Q3: 5×5 Gridworld — Value Iteration (γ = 0.99)**\n",
    "\n",
    "**Goal**\n",
    "Compute the optimal state-value function v*(s) and optimal policy π*(s).\n",
    "\n",
    "Notation (same as lecture):\n",
    "- states: s ∈ S, written as s_{i,j} (row i, column j)\n",
    "- actions: a ∈ A(s), with a1=Right, a2=Down, a3=Left, a4=Up\n",
    "- transition: s' = δ(s,a) (deterministic; invalid move ⇒ s'=s)\n",
    "- reward: R(s)\n",
    "- optimal value: v*(s)\n",
    "- optimal policy: π*(s)\n",
    "\n",
    "In this grid, the goal state is s_{4,4} (bottom-right cell).\n",
    "\n",
    "Grey states are valid but unfavorable: { s_{2,2}, s_{3,0}, s_{0,4} }."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097364c",
   "metadata": {},
   "source": [
    "#### **Reward function R(s)**\n",
    "\n",
    "R(s) =\n",
    "- +10  if s = s_{4,4} (Goal)\n",
    "- −5   if s ∈ S_grey = { s_{2,2}, s_{3,0}, s_{0,4} }\n",
    "- −1   otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf11ec51",
   "metadata": {},
   "source": [
    "#### **Value Iteration update (Bellman optimality)**\n",
    "\n",
    "For each non-terminal state s:\n",
    "\n",
    "v_{k+1}(s) = max over actions a of  [ R(s') + γ v_k(s') ] , where  s' = δ(s,a)\n",
    "\n",
    "Stop when the maximum change becomes very small: max over s of | v_{k+1}(s) − v_k(s) | < θ\n",
    "\n",
    "Then extract the greedy policy: π*(s) = argmax over a of [ R(s') + γ v*(s') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96bf957",
   "metadata": {},
   "source": [
    "#### **Running the implementation**\n",
    "\n",
    "The Value Iteration algorithm for the 5×5 Gridworld is implemented in a separate Python file\n",
    "`q3_gridworld_vi_oop.py`.\n",
    "\n",
    "We import the function `run_q3_value_iteration`, which:\n",
    "\n",
    "- Builds the Gridworld MDP (states, actions, transition δ, reward R)\n",
    "- Runs Value Iteration using γ = 0.99\n",
    "- Computes the optimal state-value function v*(s)\n",
    "- Computes the greedy optimal policy π*(s)\n",
    "- Logs how the value matrix V_k (and policy π_k) change over iterations k\n",
    "- Saves readable matrix snapshots and numeric values to log files\n",
    "\n",
    "This notebook only **calls** the function and displays results. The algorithm itself is **not duplicated here**, following good software design practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd0b742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root set to: C:\\Users\\user\\1557_VSC\\AI_Sem2\\Reinforcement Learning Programming\\CSCN8020_Assignment1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "p = Path().resolve()\n",
    "while p != p.parent and not (p / \"src\").exists():\n",
    "    p = p.parent\n",
    "\n",
    "sys.path.insert(0, str(p))\n",
    "print(\"Project root set to:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e11c7",
   "metadata": {},
   "source": [
    "#### **Executing Value Iteration and viewing results**\n",
    "\n",
    "We now run Value Iteration on the 5×5 Gridworld using:\n",
    "\n",
    "- discount factor γ = 0.99\n",
    "- a very small convergence threshold θ\n",
    "- a large maximum iteration limit to ensure convergence\n",
    "\n",
    "During execution:\n",
    "\n",
    "- V_k (the state-value matrix at iteration k) is updated repeatedly\n",
    "- At selected iterations k, the entire 5×5 value matrix is logged\n",
    "- A snapshot of the matrix values is also saved to a CSV file for analysis or plotting\n",
    "\n",
    "The function returns:\n",
    "\n",
    "- the Gridworld MDP object\n",
    "- the optimal state-value function V*\n",
    "- the optimal policy π*\n",
    "- the number of iterations required for convergence\n",
    "- the path to the human-readable log file\n",
    "- the path to the CSV file containing numeric snapshots\n",
    "\n",
    "After convergence, we print:\n",
    "\n",
    "- the total number of iterations taken\n",
    "- the final optimal value function V*\n",
    "- the final greedy optimal policy π*\n",
    "- the locations of the saved log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d38537dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same V*? True\n",
      "Same π*? True\n",
      "Standard: 9 iters, 0.030674 sec\n",
      "In-place: 9 iters, 0.034413 sec\n",
      "\n",
      "V* (standard):\n",
      "\n",
      "  2.53    3.56    4.61    5.67    6.73*\n",
      "  3.56    4.61    5.67    6.73    7.81 \n",
      "  4.61    5.67    6.73*   7.81    8.90 \n",
      "  5.67*   6.73    7.81    8.90   10.00 \n",
      "  6.73    7.81    8.90   10.00     G   \n",
      "\n",
      "π* (standard):\n",
      "\n",
      " →   →   →   ↓   ↓ \n",
      " →   →   →   →   ↓ \n",
      " →   ↓   →   →   ↓ \n",
      " →   →   →   →   ↓ \n",
      " →   →   →   →   G \n"
     ]
    }
   ],
   "source": [
    "from src.q3.gridworld_vi_oop import run_q3_variations\n",
    "\n",
    "out = run_q3_variations(gamma=0.99, theta=1e-8, max_iters=200000, snapshot_every=50)\n",
    "mdp = out[\"mdp\"]\n",
    "\n",
    "print(\"Same V*?\", out[\"same_V\"])\n",
    "print(\"Same π*?\", out[\"same_pi\"])\n",
    "print(\"Standard:\", out[\"iters_std\"], \"iters,\", round(out[\"time_std\"], 6), \"sec\")\n",
    "print(\"In-place:\", out[\"iters_ip\"], \"iters,\", round(out[\"time_ip\"], 6), \"sec\")\n",
    "\n",
    "print(\"\\nV* (standard):\\n\")\n",
    "print(mdp.format_V(out[\"V_std\"], decimals=2))\n",
    "\n",
    "print(\"\\nπ* (standard):\\n\")\n",
    "print(mdp.format_pi(out[\"pi_std\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f2491a",
   "metadata": {},
   "source": [
    "#### **TASK 2**\n",
    "\n",
    "| Aspect                   | Standard Value Iteration (Synchronous)                | In-Place Value Iteration                            |    \n",
    "| ------------------------ | ----------------------------------------------------- | --------------------------------------------------- | \n",
    "| Update style             | Uses a separate (V_{\\text{new}}) table each iteration | Updates (V(s)) directly and reuses updated values   |       \n",
    "| Converged V*             | Yes (matches exactly)                                 | Yes (matches exactly)                               |   \n",
    "| Converged π*             | Yes (matches exactly)                                 | Yes (matches exactly)                               |    \n",
    "| Same V*?                 | True                                                  | True                                                |\n",
    "| Same π*?                 | True                                                  | True                                                |\n",
    "| Iterations (k)           | 9                                                     | 9                                                   |       \n",
    "| Optimization time        | ~0.024 seconds (this run)                             | ~0.033 seconds (this run)                           |       \n",
    "| Interpretation           | Slightly faster in this run                           | Slightly slower here due to small state space       |     \n",
    "| Computational complexity | O(K · number_of_states · number_of_actions)           | S                                                   | \n",
    "| Practical note           | Stable and easy to reason about                       | Can converge in fewer iterations on larger problems |      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263965f",
   "metadata": {},
   "source": [
    "#### **Analysis/Conclusion**\n",
    "\n",
    "In this task, we solved the 5×5 Gridworld using Value Iteration to find the best long-term strategy for reaching the goal while avoiding penalty (grey) states. Each grid cell represents a state, and Value Iteration repeatedly updates a numerical value for each state that reflects how good it is to be there if the agent behaves optimally. We followed the assignment’s reward-on-arrival rule: entering the goal gives +10, entering a grey cell gives −5, and entering any other cell gives −1. With a high discount factor of γ = 0.99, the agent strongly considers future rewards rather than only immediate outcomes.\n",
    "\n",
    "Using the standard (synchronous) Value Iteration approach, the algorithm converged in 9 iterations, meaning the values stopped changing in any meaningful way and satisfied the Bellman optimality condition. The final optimal value function V* forms a clear gradient that increases toward the goal, while grey states have lower values due to their penalty. The corresponding optimal policy π* mostly moves Right and Down toward the goal and avoids grey states whenever possible, which matches intuitive expectations.\n",
    "\n",
    "To complete Task 2, we also implemented In-Place Value Iteration, where state values are updated directly and immediately reused during the same sweep. We verified that both methods reach the same optimal solution, with Same V = True* and Same π = True*. In our experiment, both approaches converged in 9 iterations, and their runtimes were very similar, with the standard version being slightly faster in this particular run. This small difference is expected given the very small state space.\n",
    "\n",
    "Both methods have the same computational complexity, O(K · |S| · |A|), but in-place updates can sometimes reduce the number of iterations required in larger or more complex environments. Overall, these results confirm that Value Iteration reliably finds an optimal policy for this finite MDP, and that the in-place variation provides a valid alternative with comparable performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
