{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04de5311",
   "metadata": {},
   "source": [
    "### **Q3: 5√ó5 Gridworld ‚Äî Value Iteration (Œ≥ = 0.99)**\n",
    "\n",
    "**Goal**\n",
    "Compute the optimal state-value function v*(s) and optimal policy œÄ*(s).\n",
    "\n",
    "Notation (same as lecture):\n",
    "- states: s ‚àà S, written as si,j  (row i, column j)\n",
    "- actions: a ‚àà A(s), with a1=Right, a2=Down, a3=Left, a4=Up\n",
    "- transition: s' = Œ¥(s,a) (deterministic; invalid move ‚áí s'=s)\n",
    "- Discount factor: ùõæ=0.99\n",
    "- reward: R(s)\n",
    "- optimal value: v*(s)\n",
    "- optimal policy: œÄ*(s)\n",
    "\n",
    "In this grid, the goal state is s_{4,4} (bottom-right cell).\n",
    "\n",
    "Grey states are valid but unfavorable: {s2,2‚Äã,s3,0‚Äã,s0,4‚Äã}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097364c",
   "metadata": {},
   "source": [
    "#### **Reward function R(s)**\n",
    "\n",
    "Reward function R(s) \n",
    "Reward is given on arrival: r = R(s‚Ä≤)\n",
    "- +10  if s = s_{4,4} (Goal)\n",
    "- ‚àí5   if s ‚àà S_grey = { s_{2,2}, s_{3,0}, s_{0,4} }\n",
    "- ‚àí1   otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf11ec51",
   "metadata": {},
   "source": [
    "#### **Value Iteration update (Bellman optimality)**\n",
    "\n",
    "For each non-terminal state s: vk+1‚Äã(s)=a‚ààA(s)max‚Äã[R(s‚Ä≤)+Œ≥vk‚Äã(s‚Ä≤)], where s‚Ä≤ = Œ¥(s,a).\n",
    "\n",
    "Stop when the maximum change becomes very small: maxs‚Äã‚à£vk+1‚Äã(s)‚àívk‚Äã(s)‚à£<Œ∏\n",
    "\n",
    "Then extract the greedy policy: œÄ*(s) = argmax over a of [ R(s') + Œ≥ v*(s') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96bf957",
   "metadata": {},
   "source": [
    "#### **Running the implementation**\n",
    "\n",
    "The Value Iteration algorithm for the 5√ó5 Gridworld is implemented in separate Python modules:\n",
    "* gridworld.py (environment definition: states, actions, transition Œ¥, reward R)\n",
    "* value_iteration_solved.py (standard synchronous Value Iteration)\n",
    "* value_iteration_inplace.py (in-place Value Iteration)\n",
    "* vi_logger.py (logging intermediate matrices and convergence data)\n",
    "\n",
    "The notebook imports and runs the appropriate function to:\n",
    "\n",
    "- Builds the Gridworld MDP (states, actions, transition Œ¥, reward R)\n",
    "- Runs Value Iteration using Œ≥ = 0.99\n",
    "- Computes the optimal state-value function v*(s)\n",
    "- Computes the greedy optimal policy œÄ*(s)\n",
    "- Logs how the value matrix V_k (and policy œÄ_k) change over iterations k\n",
    "- Saves readable matrix snapshots and numeric values to log files\n",
    "\n",
    "TThis notebook only calls the functions and displays results. The algorithm itself is not duplicated here, following good software design practice and separation of concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67367213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT = C:\\Users\\user\\1557_VSC\\AI_Sem2\\Reinforcement Learning Programming\\CSCN8020_Assignment1\n",
      "Q3_DIR = C:\\Users\\user\\1557_VSC\\AI_Sem2\\Reinforcement Learning Programming\\CSCN8020_Assignment1\\src\\q3\n",
      "LOG_DIR = C:\\Users\\user\\1557_VSC\\AI_Sem2\\Reinforcement Learning Programming\\CSCN8020_Assignment1\\logs\\q3\n"
     ]
    }
   ],
   "source": [
    "# === Q3 Setup (Lecture 3 DP files) ===\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Resolve project root (folder that contains \"src\")\n",
    "HERE = Path.cwd().resolve()\n",
    "ROOT = HERE\n",
    "while not (ROOT / \"src\").exists() and ROOT != ROOT.parent:\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "Q3_DIR = ROOT / \"src\" / \"q3\"\n",
    "if str(Q3_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(Q3_DIR))\n",
    "\n",
    "# Imports from lec3-style Q3 folder\n",
    "from gridworld import GridWorld\n",
    "from value_iteration_agent import Agent\n",
    "from vi_logger import VILogger\n",
    "\n",
    "ENV_SIZE = 5\n",
    "GAMMA = 0.99\n",
    "THETA = 1e-8\n",
    "MAX_ITERS = 200000\n",
    "\n",
    "# Where logs should go (project-level)\n",
    "LOG_DIR = ROOT / \"logs\" / \"q3\"\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT =\", ROOT)\n",
    "print(\"Q3_DIR =\", Q3_DIR)\n",
    "print(\"LOG_DIR =\", LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e11c7",
   "metadata": {},
   "source": [
    "#### **Executing Value Iteration and viewing results**\n",
    "\n",
    "We now run Value Iteration on the 5√ó5 Gridworld using:\n",
    "\n",
    "- discount factor Œ≥ = 0.99\n",
    "- a very small convergence threshold Œ∏\n",
    "- a large maximum iteration limit to ensure convergence\n",
    "\n",
    "*We evaluate two implementations:*\n",
    "* Standard (synchronous) value iteration: uses a temporary copy ùëânew each sweep.\n",
    "* In-place value iteration: updates ùëâ directly and immediately reuses updated values.\n",
    "\n",
    "*During execution:*\n",
    "- The state-value matrix Vk is updated repeatedly using the Bellman optimality backup.\n",
    "- At selected sweeps ùëò, the full 5√ó5 value table is written to a human-readable .log.\n",
    "- Numeric snapshots are also written to a .csv file for analysis/plotting.\n",
    "\n",
    "*After convergence, we report:*\n",
    "- the number of sweeps (iterations) taken to converge\n",
    "- the final optimal state-value function V*\n",
    "- the final greedy optimal policy œÄ*\n",
    "- The file paths for the generated .log and .csv outputs\n",
    "\n",
    "*Note:* Value Iteration is a Dynamic Programming algorithm and is not episode-based. Here, convergence is measured in terms of sweeps (iterations) until the maximum update difference falls below ùúÉ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "576b385f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward list length: 25\n",
      "Reward list (first 10): [-1.0, -1.0, -1.0, -1.0, -5.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n",
      "\n",
      "Goal state: (4, 4) Reward: 10.0\n",
      "Grey states: {(0, 4), (3, 0), (2, 2)}\n",
      "Grey rewards: [-5.0, -5.0, -5.0]\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Reward function as a LIST (reward_list)\n",
    "\n",
    "env = GridWorld(ENV_SIZE)\n",
    "R_list = env.get_reward_list()\n",
    "\n",
    "print(\"Reward list length:\", len(R_list))\n",
    "print(\"Reward list (first 10):\", R_list[:10])\n",
    "\n",
    "goal = (4, 4)\n",
    "grey_states = {(2, 2), (3, 0), (0, 4)}\n",
    "\n",
    "to_idx = lambda s: s[0] * ENV_SIZE + s[1]  # row-major index\n",
    "\n",
    "print(\"\\nGoal state:\", goal, \"Reward:\", R_list[to_idx(goal)])\n",
    "print(\"Grey states:\", grey_states)\n",
    "print(\"Grey rewards:\", [R_list[to_idx(s)] for s in sorted(grey_states)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79d7e1",
   "metadata": {},
   "source": [
    "#### **Executing Value Iteration and Viewing Results**\n",
    "\n",
    "We run Value Iteration on the 5√ó5 Gridworld using:\n",
    "* discount factor ùõæ=0.99\n",
    "* convergence threshold Œ∏\n",
    "* A sufficiently large maximum sweep limit to ensure convergence\n",
    "\n",
    "*Two implementations are executed:*\n",
    "* Standard (synchronous) Value Iteration using a temporary copy ùëânew each sweep\n",
    "* In-place Value Iteration updating ùëâ directly (Gauss‚ÄìSeidel style)\n",
    "\n",
    "During execution, the scripts output:\n",
    "* The final optimal value function ùëâ‚àó\n",
    "* The optimal greedy policy ùúã‚àó\n",
    "* The number of sweeps (iterations) required for convergence\n",
    "* Runtime measurements\n",
    "* Detailed intermediate snapshots saved as .log and .csv files under logs/q3/\n",
    "\n",
    "Note: Value Iteration is a Dynamic Programming method and is not episode-based. Convergence is measured in terms of sweeps (iterations) until the maximum value update falls below the threshold ùúÉ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f529df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: C:\\Users\\user\\1557_VSC\\AI_Sem2\\Reinforcement Learning Programming\\CSCN8020_Assignment1\\src\\q3\\value_iteration_solved.py\n",
      "\n",
      "Running: C:\\Users\\user\\1557_VSC\\AI_Sem2\\Reinforcement Learning Programming\\CSCN8020_Assignment1\\src\\q3\\value_iteration_inplace.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['c:\\\\Users\\\\user\\\\1557_VSC\\\\AI_Sem2\\\\Reinforcement Learning Programming\\\\CSCN8020_Assignment1\\\\.venv\\\\Scripts\\\\python.exe', 'C:\\\\Users\\\\user\\\\1557_VSC\\\\AI_Sem2\\\\Reinforcement Learning Programming\\\\CSCN8020_Assignment1\\\\src\\\\q3\\\\value_iteration_inplace.py'], returncode=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Run Task 1 (Standard VI) + Task 2 (In-place VI) using lec3 scripts ===\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "std_script = ROOT / \"src\" / \"q3\" / \"value_iteration_solved.py\"\n",
    "ip_script  = ROOT / \"src\" / \"q3\" / \"value_iteration_inplace.py\"\n",
    "\n",
    "print(\"Running:\", std_script)\n",
    "subprocess.run([sys.executable, str(std_script)], check=True, cwd=str(Q3_DIR))\n",
    "\n",
    "print(\"\\nRunning:\", ip_script)\n",
    "subprocess.run([sys.executable, str(ip_script)], check=True, cwd=str(Q3_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f2491a",
   "metadata": {},
   "source": [
    "#### **TASK 2 ‚Äî Value Iteration Variations (Standard vs In-Place)**\n",
    "\n",
    "| Aspect | Standard Value Iteration (Synchronous) | In-Place Value Iteration |\n",
    "|---|---|---|\n",
    "| Update style | Uses a separate V_new array each sweep | Updates V(s) directly and immediately reuses updated values |\n",
    "| Converged to V* | Yes | Yes |\n",
    "| Converged to œÄ* | Yes | Yes |\n",
    "| Same V*? | True (within tolerance) | True |\n",
    "| Same œÄ*? | True (exact match) | True |\n",
    "| Iterations (sweeps) to converge | 9 | 9 |\n",
    "| Runtime (seconds) | 0.069475 | 0.142434 |\n",
    "| ‚ÄúEpisodes‚Äù note | DP Value Iteration is not episode-based. Here, we report iterations/sweeps until convergence. | Same interpretation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b07f1",
   "metadata": {},
   "source": [
    "#### **Computational Complexity**\n",
    "\n",
    "Each sweep evaluates all states and all actions, so total time complexity is: O(K‚ãÖ‚à£S‚à£‚ãÖ‚à£A‚à£)\n",
    "\n",
    "where ùêæ is the number of sweeps to convergence. Space complexity is: O(‚à£S‚à£)\n",
    "\n",
    "Standard Value Iteration uses an additional temporary copy ùëânew each sweep, but memory remains linear in ‚à£ùëÜ‚à£."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274821b",
   "metadata": {},
   "source": [
    "#### **Implementation Source**\n",
    "\n",
    "This solution is based on the Lecture 3 Dynamic Programming implementation (gridworld.py, value_iteration_agent.py, value_iteration_solved.py).\n",
    "\n",
    "The environment and value iteration logic follow the Bellman optimality equation. For Q3, the reward structure was modified to match the assignment requirements (goal +10, grey ‚àí5, others ‚àí1), and an in-place value iteration variant was implemented for comparison.\n",
    "\n",
    "Performance comparison was done by running both implementations with identical parameters and comparing the resulting ùëâ‚àó,ùúã‚àó, number of sweeps, and runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263965f",
   "metadata": {},
   "source": [
    "#### **Analysis/Conclusion**\n",
    "\n",
    "This problem models the 5√ó5 Gridworld as a Markov Decision Process (MDP) with:\n",
    "\n",
    "- **States**: grid cells s_{i,j} (row, column), including a goal at s_{4,4}.\n",
    "- **Actions**: right, down, left, up with transitions (invalid moves keep the agent in the same state).\n",
    "- **Rewards** (implemented as a reward list R_list):  \n",
    "  - +10 at the goal state s_{4,4}  \n",
    "  - -5 at grey states {s_{2,2}, s_{3,0}, s_{0,4}} \n",
    "  - -1 for all other non-terminal states\n",
    "\n",
    "Using the Bellman optimality update: \n",
    "V_{k+1}(s) = max_a ( R(s') + Œ≥ V_k(s')), \n",
    "\n",
    "(with deterministic s' given (s,a)), both implementations converged to the same optimal value function V* and same greedy optimal policy œÄ* (verified by the Same V* and Same œÄ* checks printed above). This gradient structure reflects the discounted future reward propagation from the terminal state through the grid, while penalty states locally reduce surrounding values.\n",
    "\n",
    "**Convergence / Performance / Complexity**\n",
    "\n",
    "Both standard (synchronous) and in-place value iteration converged to the same optimal solution (V* and œÄ*) in 9 sweeps. With snapshot logging enabled every sweep, measured runtimes were 0.069475 s (standard) and 0.142434 s (in-place); this difference is mainly due to per-iteration file I/O overhead rather than algorithmic differences. Since Value Iteration is DP (not episodic), the reported ‚Äúepisodes‚Äù correspond to sweeps until convergence. Time complexity is O(K¬∑|S|¬∑|A|) and space is O(|S|) (standard VI uses an extra temporary V array but remains linear)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
