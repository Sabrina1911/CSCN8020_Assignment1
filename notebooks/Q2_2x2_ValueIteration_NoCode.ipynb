{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8492926b",
   "metadata": {},
   "source": [
    "### **Title: CSCN8020 ‚Äì Assignment 1 ‚Äì Problem 2 (2√ó2 Gridworld)**\n",
    "\n",
    "**Goal:** \n",
    "\n",
    "Perform two iterations of Value Iteration and show the step-by-step process (without code) including:\n",
    "* MDP definition: S,A,P(s‚Ä≤|s,a),R(s),Œ≥\n",
    "* Iteration 1:\n",
    "  - initial value function v0(s)\n",
    "  - value function updates (show q0(s,a) and max)\n",
    "  - updated value function v1‚Äã(s)\n",
    "* Iteration 2:\n",
    "  - show the value function after second iteration v2(s)\n",
    "* Policy improvement: greedy policy after each iteration (tie-handling noted)\n",
    "\n",
    "*Assumption: ùõæ = 0.98*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e41c37",
   "metadata": {},
   "source": [
    "**MDP Definition**\n",
    "\n",
    "We model the 2√ó2 Gridworld as an MDP:\n",
    "* State space:                   S = {s1‚Äã,s2‚Äã,s3‚Äã,s4}\n",
    "* Action space:                  A = {up, down, left, right}\n",
    "* Transition model:              P(s‚Ä≤|s,a) = {1, if the action is valid and deterministically leads to s‚Ä≤\n",
    "                                              1, if action hits a wall and s‚Ä≤ = s\n",
    "                                              0, otherwise\n",
    "* Reward function (state-based): R(s1‚Äã) = 5, R(s2) = 10, R(s3) = 1, R(s4) = 2\n",
    "* Discount factor:               ùõæ = 0.98\n",
    "* Initial policy:                œÄ0‚Äã(s) = up ‚àÄs  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc90a5b",
   "metadata": {},
   "source": [
    "#### **Grid layout + deterministic transitions**\n",
    "\n",
    "**Grid Layout:**             \n",
    "s1‚Äã s2 \n",
    "\n",
    "s3 s4\n",
    "\n",
    "*Deterministic Next-State Mapping* s‚Ä≤ = f(s,a)\n",
    "\n",
    "* From s1 : up->s1(wall), left->s1(wall), right->s2, down->s3\n",
    "* From s2 : up->s2(wall), right->s2(wall), left->s1, down->s4\n",
    "* From s3 : down->s3(wall), left->s3(wall), up->s1, right->s4\n",
    "* From s4 : down->s4(wall), right->s4(wall), up->s2, left->s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c503a291",
   "metadata": {},
   "source": [
    "#### **Value Iteration equations in their notation**\n",
    "\n",
    "Value Iteration (Bellman Optimality Backup)\n",
    "\n",
    "* First define: qk‚Äã(s,a) = ‚àës‚Ä≤ P(s‚Ä≤|s,a) (R(s) + Œ≥ vk‚Äã (s‚Ä≤))\n",
    "\n",
    "Then update value:  vk+1‚Äã(s) = max qk‚Äã(s,a)\n",
    "\n",
    "Since transitions are deterministic (one next state ùë†‚Ä≤ with probability 1): qk‚Äã(s,a) = R(s) + Œ≥ vk‚Äã (s‚Ä≤)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5342dbb5",
   "metadata": {},
   "source": [
    "#### **Iteration 0 ‚Üí Iteration 1: initial values**\n",
    "\n",
    "*Step 1* ‚Äî Initialize ùë£0(ùë†)\n",
    "\n",
    "We initialize the value function to zero:  ùë£0(ùë†1) = ùë£0(ùë†2) = ùë£0(ùë†3) = ùë£0(ùë†4) = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265e909",
   "metadata": {},
   "source": [
    "#### **Iteration 1: compute v‚ÇÅ(s)**\n",
    "\n",
    "**Step 2 ‚Äî Compute ùëû0(ùë†,ùëé) and update v1(s)**\n",
    "\n",
    "*State s1 (reward R(s1) = 5)*\n",
    "   * up -> s‚Ä≤ = s1:\n",
    "     q0(s1,up) = 5 + 0.98 * v0(s1) = 5 + 0.98 * 0 = 5\n",
    "\n",
    "   * left -> s‚Ä≤ = s1:\n",
    "     q0(s1,left) = 5 + 0.98*0 = 5\n",
    "\n",
    "   * right -> s‚Ä≤ = s2:\n",
    "     q0(s1,right) = 5 + 0.98 * v0(s2) = 5\n",
    "\n",
    "   * down -> s‚Ä≤ = s3: \n",
    "     q0(s1,down) = 5 + 0.98 * v0(s3) = 5\n",
    "\n",
    "So: v1(s1) = max (5,5,5,5) = 5\n",
    "\n",
    "*State s2 (reward R(s2) = 10)*\n",
    "- All next-state values are 0 in ùë£0, so for  any action a: q0(s2,a) = 10 + 0.98*0 = 10\n",
    "- So: v1(s2) = 10\n",
    "\n",
    "*State s3 (reward R(s3) = 1)*\n",
    "- q0(s3,a) = 1 + 0.98*0 = 1 -> v1(s3) = 1\n",
    "\n",
    "*State s4 (reward R(s4) = 2)*\n",
    "- q0(s4,a) = 2+ 0.98 = 2 -> v1(s4) = 2\n",
    "\n",
    "**Step 3 - Update value function v1**\n",
    "\n",
    "v1(s1) = 5 , v1(s2) = 10, v1(s3) = 1, v1(s4) = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a5154",
   "metadata": {},
   "source": [
    "**Policy improvement after Iteration 1**\n",
    "\n",
    "Policy Improvement after Iteration 1 (greedy w.r.t. ùë£0)\n",
    "\n",
    "Greedy policy chooses: ùúã1(ùë†) = argmaxùëé ùëû0(ùë†,ùëé)\n",
    "\n",
    "But since ùë£0(‚ãÖ) = 0, all actions tie in every state (all ùëû0 values are equal per state).\n",
    "\n",
    "Therefore any greedy action is valid. To stay consistent with the given initial policy: œÄ1‚Äã(s) = up ‚àÄs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970626b",
   "metadata": {},
   "source": [
    "#### **Iteration 2**\n",
    "\n",
    "Iteration 2 (compute ùë£2 from ùë£1)\n",
    "\n",
    "We compute: \n",
    "* q1(s,a) = R(s) + Œ≥ v1(s‚Ä≤) \n",
    "* v2(s) = max over all actions a of q1(s,a)\n",
    "\n",
    "The discount factor used is Œ≥ = 0.98.\n",
    "\n",
    "**State s1 (reward = 5)**\n",
    "\n",
    "Next states:\n",
    "- up ‚Üí s1  \n",
    "- left ‚Üí s1  \n",
    "- right ‚Üí s2  \n",
    "- down ‚Üí s3  \n",
    "\n",
    "Compute:\n",
    "- q‚ÇÅ(s1, up) = 5 + 0.98 √ó 5 = 9.90  \n",
    "- q‚ÇÅ(s1, left) = 5 + 0.98 √ó 5 = 9.90  \n",
    "- q‚ÇÅ(s1, right) = 5 + 0.98 √ó 10 = 14.80  \n",
    "- q‚ÇÅ(s1, down) = 5 + 0.98 √ó 1 = 5.98  \n",
    "\n",
    "So:\n",
    "- v‚ÇÇ(s1) = max(9.90, 9.90, 14.80, 5.98) = 14.80  \n",
    "- Greedy action: right\n",
    "\n",
    "**State s2 (reward = 10)**\n",
    "\n",
    "Next states:\n",
    "- up ‚Üí s2  \n",
    "- right ‚Üí s2  \n",
    "- left ‚Üí s1  \n",
    "- down ‚Üí s4 \n",
    "\n",
    "Compute:\n",
    "- q‚ÇÅ(s2, up) = 10 + 0.98 √ó 10 = 19.80  \n",
    "- q‚ÇÅ(s2, right) = 10 + 0.98 √ó 10 = 19.80  \n",
    "- q‚ÇÅ(s2, left) = 10 + 0.98 √ó 5 = 14.90  \n",
    "- q‚ÇÅ(s2, down) = 10 + 0.98 √ó 2 = 11.96\n",
    "\n",
    "So:\n",
    "- v‚ÇÇ(s2) = 19.80\n",
    "- Greedy action: up or right\n",
    "\n",
    "**State s3 (reward = 1)**\n",
    "\n",
    "Next states:\n",
    "- up ‚Üí s1  \n",
    "- right ‚Üí s4  \n",
    "- left ‚Üí s3  \n",
    "- down ‚Üí s3  \n",
    "\n",
    "Compute:\n",
    "- q‚ÇÅ(s3, up) = 1 + 0.98 √ó 5 = 5.90  \n",
    "- q‚ÇÅ(s3, right) = 1 + 0.98 √ó 2 = 2.96  \n",
    "- q‚ÇÅ(s3, left) = 1 + 0.98 √ó 1 = 1.98  \n",
    "- q‚ÇÅ(s3, down) = 1 + 0.98 √ó 1 = 1.98  \n",
    "\n",
    "So:\n",
    "- v‚ÇÇ(s3) = 5.90  \n",
    "- Greedy action: up\n",
    "\n",
    "**State s4 (reward = 2)**\n",
    "\n",
    "Next states:\n",
    "- up ‚Üí s2  \n",
    "- left ‚Üí s3  \n",
    "- right ‚Üí s4  \n",
    "- down ‚Üí s4 \n",
    "\n",
    "Compute:\n",
    "- q‚ÇÅ(s4, up) = 2 + 0.98 √ó 10 = 11.80  \n",
    "- q‚ÇÅ(s4, left) = 2 + 0.98 √ó 1 = 2.98  \n",
    "- q‚ÇÅ(s4, right) = 2 + 0.98 √ó 2 = 3.96  \n",
    "- q‚ÇÅ(s4, down) = 2 + 0.98 √ó 2 = 3.96  \n",
    "\n",
    "So:\n",
    "- v‚ÇÇ(s4) = 11.80  \n",
    "- Greedy action: up\n",
    "\n",
    "**Final value function after Iteration 2**\n",
    "- v‚ÇÇ(s1) = 14.80  \n",
    "- v‚ÇÇ(s2) = 19.80  \n",
    "- v‚ÇÇ(s3) = 5.90  \n",
    "- v‚ÇÇ(s4) = 11.80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847c0c6",
   "metadata": {},
   "source": [
    "#### **Policy improvement after Iteration 2**\n",
    "\n",
    "After computing q‚ÇÅ(s, a) for each action, the greedy policy chooses the action that gives the highest q-value:\n",
    "\n",
    "- œÄ‚ÇÇ(s) = argmax over actions a of q‚ÇÅ(s, a)\n",
    "\n",
    "Using the best actions found in Iteration 2:\n",
    "\n",
    "- œÄ‚ÇÇ(s1) = right  \n",
    "- œÄ‚ÇÇ(s2) = up  (tie with right)  \n",
    "- œÄ‚ÇÇ(s3) = up  \n",
    "- œÄ‚ÇÇ(s4) = up  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a1726",
   "metadata": {},
   "source": [
    "#### **Conclusion (why this is correct)**\n",
    "\n",
    "- In Iteration 1, we started with v‚ÇÄ(s) = 0 for all states.  \n",
    "  Because of that, the update simplifies to v‚ÇÅ(s) = R(s).  \n",
    "  (The discounted next-state term becomes zero since v‚ÇÄ is zero everywhere.)\n",
    "\n",
    "- In Iteration 2, the values increase because we now add the discounted next-state value Œ≥ ¬∑ v‚ÇÅ(s‚Ä≤).  \n",
    "  So actions that move toward a state with higher v‚ÇÅ become better.\n",
    "\n",
    "- State s2 has the highest reward (10), and actions like up/right from s2 keep the agent in s2 (wall), so s2 produces the largest updated value.\n",
    "\n",
    "- The greedy policy after Iteration 2 therefore prefers:\n",
    "  - moving right from s1 to reach s2\n",
    "  - moving up from s4 to reach s2\n",
    "  - moving up from s3 to reach s1 (and then toward s2)\n",
    "\n",
    "This matches the idea of maximizing discounted return with Œ≥ = 0.98."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d178082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02a99922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1: {'s1': 5.0, 's2': 10.0, 's3': 1.0, 's4': 2.0}\n",
      "V2: {'s1': 14.8, 's2': 19.8, 's3': 5.9, 's4': 11.8}\n",
      "pi2: {'s1': 'right', 's2': 'up', 's3': 'up', 's4': 'up'}\n"
     ]
    }
   ],
   "source": [
    "from src.q2.mdp_analysis import value_iteration_two_iters_with_log\n",
    "\n",
    "V1, V2, pi2 = value_iteration_two_iters_with_log(\n",
    "    out_log_path=\"logs/q2/q2_mdp_analysis_log.txt\",\n",
    "    gamma=0.98\n",
    ")\n",
    "\n",
    "print(\"V1:\", V1)\n",
    "print(\"V2:\", V2)\n",
    "print(\"pi2:\", pi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb1df93",
   "metadata": {},
   "source": [
    "#### **Conlcusion/Analysis**\n",
    "\n",
    "A small 2√ó2 Gridworld Markov Decision Process was analyzed using two iterations of value iteration. The goal was not to find a fully optimal solution, but to clearly show how values and actions evolve when the Bellman update rule is applied step by step.\n",
    "\n",
    "The environment is simple and deterministic. Each state has a fixed reward, and every action leads to a predictable next state. A high discount factor of 0.98 was used, which means that future rewards that are close in time are valued almost as much as immediate rewards, while rewards further away have slightly less influence.\n",
    "\n",
    "**Understanding Iteration 1**\n",
    "\n",
    "The value function was initially set to zero for all states. Because of this, the first iteration has a very straightforward outcome. With no future information available yet, each state‚Äôs value becomes equal to its immediate reward.\n",
    "\n",
    "This is exactly what the results show:\n",
    "* s1 gets a value of 5\n",
    "* s2 gets a value of 10\n",
    "* s3 gets a value of 1\n",
    "* s4 gets a value of 2\n",
    "\n",
    "At this point, the agent has no reason to prefer one direction over another based on long-term outcomes. Each state only reflects what it gives immediately. This confirms that the initialization and reward setup are working correctly.\n",
    "\n",
    "**Understanding Iteration 2**\n",
    "\n",
    "In the second iteration, the values from Iteration 1 are used to evaluate future outcomes. Now, actions matter because they determine which state the agent will reach next.\n",
    "\n",
    "A clear pattern emerges:\n",
    "* State s2 has the highest reward and also allows the agent to remain in s2 when moving up or right.\n",
    "* Because of this, s2‚Äôs value increases significantly.\n",
    "* States that can move toward s2 also improve.\n",
    "  - s1 improves by moving right to s2.\n",
    "  - s4 improves by moving up to s2.\n",
    "* State s3 improves more slowly because it is farther from s2.\n",
    "\n",
    "After Iteration 2, the values increase in a way that reflects how close each state is to the high-reward state s2. This demonstrates how value iteration propagates reward information through the state space.\n",
    "\n",
    "**Policy Behavior**\n",
    "\n",
    "The greedy policy derived after Iteration 2 consistently chooses actions that move the agent toward s2. This makes intuitive sense because s2 offers the highest long-term return. The policy is not forced or hard-coded; it naturally emerges from the value updates.\n",
    "\n",
    "**Final Conclusion**\n",
    "\n",
    "Overall, the results clearly demonstrate how value iteration works in practice. Values start simple, then gradually incorporate future rewards as iterations progress. High-reward states influence nearby states first, and optimal actions emerge naturally from the value function.\n",
    "\n",
    "The final values, Q-table, matrix view, and policy grid are all consistent with one another and with the Bellman update principle. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
