========================================================================
Q2 — 2x2 Gridworld — Value Iteration (2 iterations)
Assumption: gamma = 0.98
States: s1,s2,s3,s4 | Actions: up,down,left,right
Update: q_k(s,a)=R(s)+gamma*V_k(s')  (deterministic transitions)
Then:   V_{k+1}(s)=max_a q_k(s,a)
========================================================================

INITIAL VALUES (V0)
  V0(s1) = 0.0000
  V0(s2) = 0.0000
  V0(s3) = 0.0000
  V0(s4) = 0.0000

------------------------------------------------------------------------
ITERATION 1 (compute V1 from V0)
------------------------------------------------------------------------

State s1: R(s1)=5.0000
  a=up    -> s'=s1 | q0(s1,up) = R(s1) + γ V0(s1)
              = 5.0000 + 0.98*0.0000 = 5.0000
  a=down  -> s'=s3 | q0(s1,down) = R(s1) + γ V0(s3)
              = 5.0000 + 0.98*0.0000 = 5.0000
  a=left  -> s'=s1 | q0(s1,left) = R(s1) + γ V0(s1)
              = 5.0000 + 0.98*0.0000 = 5.0000
  a=right -> s'=s2 | q0(s1,right) = R(s1) + γ V0(s2)
              = 5.0000 + 0.98*0.0000 = 5.0000
  => V1(s1) = max_a q0(s1,a) = 5.0000  (greedy a*: up)

State s2: R(s2)=10.0000
  a=up    -> s'=s2 | q0(s2,up) = R(s2) + γ V0(s2)
              = 10.0000 + 0.98*0.0000 = 10.0000
  a=down  -> s'=s4 | q0(s2,down) = R(s2) + γ V0(s4)
              = 10.0000 + 0.98*0.0000 = 10.0000
  a=left  -> s'=s1 | q0(s2,left) = R(s2) + γ V0(s1)
              = 10.0000 + 0.98*0.0000 = 10.0000
  a=right -> s'=s2 | q0(s2,right) = R(s2) + γ V0(s2)
              = 10.0000 + 0.98*0.0000 = 10.0000
  => V1(s2) = max_a q0(s2,a) = 10.0000  (greedy a*: up)

State s3: R(s3)=1.0000
  a=up    -> s'=s1 | q0(s3,up) = R(s3) + γ V0(s1)
              = 1.0000 + 0.98*0.0000 = 1.0000
  a=down  -> s'=s3 | q0(s3,down) = R(s3) + γ V0(s3)
              = 1.0000 + 0.98*0.0000 = 1.0000
  a=left  -> s'=s3 | q0(s3,left) = R(s3) + γ V0(s3)
              = 1.0000 + 0.98*0.0000 = 1.0000
  a=right -> s'=s4 | q0(s3,right) = R(s3) + γ V0(s4)
              = 1.0000 + 0.98*0.0000 = 1.0000
  => V1(s3) = max_a q0(s3,a) = 1.0000  (greedy a*: up)

State s4: R(s4)=2.0000
  a=up    -> s'=s2 | q0(s4,up) = R(s4) + γ V0(s2)
              = 2.0000 + 0.98*0.0000 = 2.0000
  a=down  -> s'=s4 | q0(s4,down) = R(s4) + γ V0(s4)
              = 2.0000 + 0.98*0.0000 = 2.0000
  a=left  -> s'=s3 | q0(s4,left) = R(s4) + γ V0(s3)
              = 2.0000 + 0.98*0.0000 = 2.0000
  a=right -> s'=s4 | q0(s4,right) = R(s4) + γ V0(s4)
              = 2.0000 + 0.98*0.0000 = 2.0000
  => V1(s4) = max_a q0(s4,a) = 2.0000  (greedy a*: up)

UPDATED VALUES (V1)
  V1(s1) = 5.0000
  V1(s2) = 10.0000
  V1(s3) = 1.0000
  V1(s4) = 2.0000

------------------------------------------------------------------------
ITERATION 2 (compute V2 from V1)
------------------------------------------------------------------------

State s1: R(s1)=5.0000
  a=up    -> s'=s1 | q1(s1,up) = R(s1) + γ V1(s1)
              = 5.0000 + 0.98*5.0000 = 9.9000
  a=down  -> s'=s3 | q1(s1,down) = R(s1) + γ V1(s3)
              = 5.0000 + 0.98*1.0000 = 5.9800
  a=left  -> s'=s1 | q1(s1,left) = R(s1) + γ V1(s1)
              = 5.0000 + 0.98*5.0000 = 9.9000
  a=right -> s'=s2 | q1(s1,right) = R(s1) + γ V1(s2)
              = 5.0000 + 0.98*10.0000 = 14.8000
  => V2(s1) = max_a q1(s1,a) = 14.8000  (greedy a*: right)

State s2: R(s2)=10.0000
  a=up    -> s'=s2 | q1(s2,up) = R(s2) + γ V1(s2)
              = 10.0000 + 0.98*10.0000 = 19.8000
  a=down  -> s'=s4 | q1(s2,down) = R(s2) + γ V1(s4)
              = 10.0000 + 0.98*2.0000 = 11.9600
  a=left  -> s'=s1 | q1(s2,left) = R(s2) + γ V1(s1)
              = 10.0000 + 0.98*5.0000 = 14.9000
  a=right -> s'=s2 | q1(s2,right) = R(s2) + γ V1(s2)
              = 10.0000 + 0.98*10.0000 = 19.8000
  => V2(s2) = max_a q1(s2,a) = 19.8000  (greedy a*: up)

State s3: R(s3)=1.0000
  a=up    -> s'=s1 | q1(s3,up) = R(s3) + γ V1(s1)
              = 1.0000 + 0.98*5.0000 = 5.9000
  a=down  -> s'=s3 | q1(s3,down) = R(s3) + γ V1(s3)
              = 1.0000 + 0.98*1.0000 = 1.9800
  a=left  -> s'=s3 | q1(s3,left) = R(s3) + γ V1(s3)
              = 1.0000 + 0.98*1.0000 = 1.9800
  a=right -> s'=s4 | q1(s3,right) = R(s3) + γ V1(s4)
              = 1.0000 + 0.98*2.0000 = 2.9600
  => V2(s3) = max_a q1(s3,a) = 5.9000  (greedy a*: up)

State s4: R(s4)=2.0000
  a=up    -> s'=s2 | q1(s4,up) = R(s4) + γ V1(s2)
              = 2.0000 + 0.98*10.0000 = 11.8000
  a=down  -> s'=s4 | q1(s4,down) = R(s4) + γ V1(s4)
              = 2.0000 + 0.98*2.0000 = 3.9600
  a=left  -> s'=s3 | q1(s4,left) = R(s4) + γ V1(s3)
              = 2.0000 + 0.98*1.0000 = 2.9800
  a=right -> s'=s4 | q1(s4,right) = R(s4) + γ V1(s4)
              = 2.0000 + 0.98*2.0000 = 3.9600
  => V2(s4) = max_a q1(s4,a) = 11.8000  (greedy a*: up)

UPDATED VALUES (V2)
  V2(s1) = 14.8000
  V2(s2) = 19.8000
  V2(s3) = 5.9000
  V2(s4) = 11.8000

GREEDY POLICY after Iteration 2 (greedy w.r.t V1)
  π2(s1) = right
  π2(s2) = up
  π2(s3) = up
  π2(s4) = up

Q1(s,a) table (using V1, gamma = 0.98)
             up     down     left    right
s1        9.90    5.98    9.90   14.80
s2       19.80   11.96   14.90   19.80
s3        5.90    1.98    1.98    2.96
s4       11.80    3.96    2.98    3.96

========================================================================
MATRIX / GRID REPRESENTATION (for report-style viewing)
========================================================================
V0 matrix:
Layout: [ s1  s2 ; s3  s4 ]
[   0.00    0.00 ]
[   0.00    0.00 ]

V1 matrix:
Layout: [ s1  s2 ; s3  s4 ]
[   5.00   10.00 ]
[   1.00    2.00 ]

V2 matrix:
Layout: [ s1  s2 ; s3  s4 ]
[  14.80   19.80 ]
[   5.90   11.80 ]

Greedy policy grid after Iteration 2 (π2):
Policy grid (arrows):
[ s1 →   s2 ↑ ]
[ s3 ↑   s4 ↑ ]

CONCLUSION
  - Iteration 1 collapses to V1(s)=R(s) because V0 is initialized to 0.
  - Iteration 2 propagates value through γ*V1(s'), producing higher values near s2 (reward 10).
========================================================================
