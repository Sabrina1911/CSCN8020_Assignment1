==============================
Q4 - Off-policy Monte Carlo (Weighted IS)
Timestamp: 2026-02-11 19:25:52
==============================

Environment: 5x5, goal=(4, 4), grey=[(2, 2), (3, 0), (0, 4)], gamma=0.9
Run args: episodes=100000, seed=7, max_steps=200
VI per-iteration logging: deltas
MC checkpoint logging every: 10000 episodes

--- Value Iteration convergence trace ---
VI iter 01: delta=10.000000000000
VI iter 02: delta=9.000000000000
VI iter 03: delta=8.100000000000
VI iter 04: delta=7.290000000000
VI iter 05: delta=6.561000000000
VI iter 06: delta=5.904900000000
VI iter 07: delta=5.314410000000
VI iter 08: delta=4.782969000000
VI iter 09: delta=0.000000000000

MC checkpoint @ episode 10,000: steps_so_far=781,109, max|ΔV| since last checkpoint=10.000000
MC checkpoint @ episode 20,000: steps_so_far=1,578,525, max|ΔV| since last checkpoint=0.010762
MC checkpoint @ episode 30,000: steps_so_far=2,364,241, max|ΔV| since last checkpoint=0.011800
MC checkpoint @ episode 40,000: steps_so_far=3,143,280, max|ΔV| since last checkpoint=0.012310
MC checkpoint @ episode 50,000: steps_so_far=3,931,466, max|ΔV| since last checkpoint=0.004503
MC checkpoint @ episode 60,000: steps_so_far=4,718,056, max|ΔV| since last checkpoint=0.005169
MC checkpoint @ episode 70,000: steps_so_far=5,493,881, max|ΔV| since last checkpoint=0.002566
MC checkpoint @ episode 80,000: steps_so_far=6,281,281, max|ΔV| since last checkpoint=0.006150
MC checkpoint @ episode 90,000: steps_so_far=7,064,746, max|ΔV| since last checkpoint=0.008928
MC checkpoint @ episode 100,000: steps_so_far=7,847,930, max|ΔV| since last checkpoint=0.011293

--- Monte Carlo (estimated - final) ---
Episodes sampled: 100000
Total transitions used: 7847930
Runtime (MC): 11.8502 seconds

Estimated V (from Q):
 -0.44   0.62   1.78   3.09   4.55
  0.62   1.79   3.10   4.55   6.18
  1.79   3.10   4.56   6.18   7.99
  3.10   4.55   6.18   7.99  10.00
  4.55   6.18   7.99  10.00    G   

Greedy target policy (from Q):
 ↓  →  →  ↓  ↓ 
 ↓  →  →  ↓  ↓ 
 →  ↓  ↓  ↓  ↓ 
 →  →  →  →  ↓ 
 →  →  →  →  G 

--- Value Iteration (baseline - final) ---
Iterations: 9
Runtime (VI): 0.0011 seconds

Optimal V*:
 -0.43   0.63   1.81   3.12   4.58
  0.63   1.81   3.12   4.58   6.20
  1.81   3.12   4.58   6.20   8.00
  3.12   4.58   6.20   8.00  10.00
  4.58   6.20   8.00  10.00    G   

Optimal policy pi*:
 →  →  →  ↓  ↓ 
 →  →  →  →  ↓ 
 →  ↓  →  →  ↓ 
 →  →  →  →  ↓ 
 →  →  →  →  G 

--- Quick comparison ---
Max |V_MC - V*|: 0.0312

Optimization time & computation (high level):
- Value Iteration: model-based sweeps all states each iteration. One sweep ≈ O(|S|·|A|). Total ≈ O(K·|S|·|A|).
- Off-policy MC (IS): model-free, uses sampled episodes. Total ≈ O(E·L) where E=episodes and L=avg episode length (capped).
- MC typically needs many episodes for stable estimates; importance sampling can have high variance, so checkpoint logs help visualize progress.
