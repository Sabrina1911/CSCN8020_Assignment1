## CSCN8020 â€“ Assignment 1
### Reinforcement Learning Programming

**Course:** CSCN8020 â€“ Reinforcement Learning Programming  
**Assignment:** Assignment 1  
**Student:** Sabrina Ronnie George Karippatt  

---

#### Assignment Overview

This assignment explores foundational reinforcement learning concepts through **Markov Decision Processes (MDPs)** and **Monte Carlo (MC)** methods. The focus is on modeling environments, computing value functions, and comparing **model-based** and **model-free** approaches using Gridworld examples.

The assignment consists of four problems:

1. **Problem 1:** MDP design for a Pick-and-Place Robot  
2. **Problem 2:** Manual Value Iteration on a 2Ã—2 Gridworld  
3. **Problem 3:** Value Iteration on a 5Ã—5 Gridworld (MDP-based)  
4. **Problem 4:** Off-policy Monte Carlo with Importance Sampling  

---

#### Folder Structure

```
CSCN8020_Assignment1/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ q1/
â”‚   â”‚   â”œâ”€â”€ demo_rollout.py          # Q1 â€“ Pick-and-Place rollout driver
â”‚   â”‚   â”œâ”€â”€ plot_rollout.py          # Q1 â€“ Plot generation
â”‚   â”‚   â””â”€â”€ pick_place_env.py        # Q1 â€“ Pick-and-Place environment (MDP)
â”‚   â”‚
â”‚   â”œâ”€â”€ q2/
â”‚   â”‚   â”œâ”€â”€ demo_q2.py               # Q2 â€“ Driver for 2Ã—2 Gridworld value iteration
â”‚   â”‚   â””â”€â”€ mdp_analysis.py          # Q2 â€“ Bellman backup logic + detailed logging
â”‚   â”‚
â”‚   â”œâ”€â”€ q3/
â”‚   â”‚   â”œâ”€â”€ gridworld.py             # Q3 â€“ 5Ã—5 Gridworld environment (Î´, R, Î³)
â”‚   â”‚   â”œâ”€â”€ value_iteration_agent.py # Q3 â€“ Shared VI agent utilities
â”‚   â”‚   â”œâ”€â”€ value_iteration_solved.py    # Q3 â€“ Standard (synchronous) Value Iteration
â”‚   â”‚   â”œâ”€â”€ value_iteration_inplace.py   # Q3 â€“ In-place (Gaussâ€“Seidel) Value Iteration
â”‚   â”‚   â””â”€â”€ vi_logger.py             # Q3 â€“ Logging + CSV snapshot utilities
â”‚   â”‚
â”‚   â””â”€â”€ q4/
â”‚       â”œâ”€â”€ offpolicy_mc_importance_sampling.py  # Q4 â€“ Off-policy MC (OIS & WIS)
â”‚       â””â”€â”€ run_q4.py                            # Q4 â€“ Runner that executes + saves outputs
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ q1/
â”‚   â”‚   â”œâ”€â”€ q1_pick_place_log.txt    # Q1 rollout log
â”‚   â”‚   â”œâ”€â”€ q1_rollout_report.pdf    # Q1 rollout summary report
â”‚   â”‚   â””â”€â”€ figures/                 # Q1 plots (reward, distance, actions)
â”‚   â”‚
â”‚   â”œâ”€â”€ q2/
â”‚   â”‚   â””â”€â”€ q2_mdp_analysis_log.txt  # Q2 step-by-step value iteration log
â”‚   â”‚
â”‚   â”œâ”€â”€ q3/                          # Q3 value iteration logs + CSV snapshots
â”‚   â”‚   â”œâ”€â”€ q3_std_value_iteration_*.log
â”‚   â”‚   â”œâ”€â”€ q3_inplace_value_iteration_*.log
â”‚   â”‚   â”œâ”€â”€ q3_std_snapshots_*.csv
â”‚   â”‚   â””â”€â”€ q3_inplace_snapshots_*.csv
â”‚   â”‚
â”‚   â””â”€â”€ q4/                          # Q4 Monte Carlo logs + CSV outputs
â”‚       â”œâ”€â”€ q4_run_*.txt
â”‚       â”œâ”€â”€ V_mc_*.csv
â”‚       â”œâ”€â”€ V_vi_*.csv
â”‚       â”œâ”€â”€ pi_mc_*.csv
â”‚       â””â”€â”€ pi_vi_*.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Q1_PickAndPlace_MDP.ipynb
â”‚   â”œâ”€â”€ Q2_2x2_ValueIteration_NoCode.ipynb
â”‚   â”œâ”€â”€ Q3_5x5_ValueIteration.ipynb
â”‚   â””â”€â”€ Q4_OffPolicy_MC_ImportanceSampling.ipynb
â”‚
â””â”€â”€ README.md

```

**Note:** Virtual environment folders (e.g., `.venv/`) are intentionally excluded from submission to keep the project portable and reproducible.

---

#### Problem Details

**Problem 1 â€“ Pick-and-Place Robot (MDP Design & Rollout Analysis)**
- The task is formulated as a **finite-horizon Markov Decision Process**
- **States:** Robot arm positions, velocities, and object status  
- **Actions:** Joint motor controls  
- **Rewards:** Encourage smooth motion, progress toward the goal, and successful task completion  
- A scripted policy rollout is executed to **validate environment dynamics and reward design**
- No learning is performed; analysis is based on **logged trajectories, plots, and a final rollout report**

**Problem 2 â€“ 2Ã—2 Gridworld (Manual Value Iteration)**
- Performs **two iterations** of Value Iteration derived manually
- Demonstrates:
  - Value function initialization
  - Bellman backups
  - Greedy policy extraction after iteration 2
- A small Python script is used **only to verify calculations and generate a step-by-step log**

**Problem 3 â€“ 5Ã—5 Gridworld (MDP Value Iteration, Î³ = 0.99)**
- Deterministic transition dynamics with reward-on-arrival
- Terminal goal state with positive reward (+10)
- Grey states with negative rewards (âˆ’5)
- Step cost of âˆ’1 for all other states
- Implemented:
  - **Standard (synchronous) Value Iteration**
  - **In-place Value Iteration**
- Both implementations converge to the same optimal state-value function Vâˆ— and optimal greedy policy ğœ‹âˆ—
- Demonstrates deterministic contraction behavior of Dynamic Programming when the full MDP model is known

**Problem 4 â€“ Off-policy Monte Carlo with Importance Sampling**
- Uses the same 5Ã—5 Gridworld as Problem 3
- **Behavior policy:** Uniform random  
- **Target policy:** Greedy with respect to learned ğ‘„(ğ‘ ,ğ‘)  
- Implements:
  - Ordinary Importance Sampling (OIS)
  - Weighted Importance Sampling (WIS)
- Estimates the action-value function Q(s,a),then derives:
  * V(s)=maxaâ€‹Q(s,a)
  * Greedy policy ğœ‹(ğ‘ )
- Monte Carlo estimates are compared against Value Iteration (Q3) as the optimal baseline
- Results show:
  * High variance and instability for OIS
  * Improved stability and convergence for WIS
  * Convergence toward the optimal solution despite being model-free

---

#### Comparison: Value Iteration vs Monte Carlo (Q3 vs Q4)

| Aspect | Value Iteration (Q3, Î³ = 0.99) | Monte Carlo (Q4, Î³ = 0.9) |
|---|---|---|
| Environment Knowledge | Full MDP model required (transition Î´ and reward R known) | Model-free (no transition model required) |
| Update Method | Bellman optimality backup applied to **all states each sweep** | Returns estimated from **sampled episodes** |
| Data Requirement | No episodes required | Requires many episodes for stable estimates |
| Iteration Unit | Sweeps over entire state space | Episodes (complete trajectories) |
| Convergence Behavior | Deterministic and fast contraction | Stochastic and sample-dependent |
| Variance | Very low (deterministic updates) | High for Ordinary IS; reduced using Weighted IS |
| Importance Sampling | Not required | Required to correct behaviorâ€“target policy mismatch |
| Computational Cost | O(K Â· \|S\| Â· \|A\|) | O(E Â· T) (episodes Ã— trajectory length) |
| Accuracy | Exact optimal solution after convergence | Approximate; approaches optimal solution asymptotically |
| Sensitivity to Î³ | Higher Î³ (0.99) emphasizes long-term reward | Lower Î³ (0.9) stabilizes Monte Carlo estimates |
| Best Use Case | Small or fully known environments | Large-scale or unknown environments |


---

#### How to Run

```bash
# Q1
python -m src.q1.demo_rollout
python -m src.q1.plot_rollout

# Q2
python -m src.q2.demo_q2

# Q3 - Standard Value Iteration
python -m src.q3.value_iteration_solved

# Q3 - In-Place Value Iteration
python -m src.q3.value_iteration_inplace

# Q4 - Off-Policy Monte Carlo with Importance Sampling
python -m src.q4.run_q4
```

**Python version:** Python 3.9+ (tested with Python 3.14)

---

## Key Takeaways

- Value Iteration efficiently computes the optimal solution when the full MDP model (states, transitions, and rewards) is known. In Q3, using Î³ = 0.99, the 5Ã—5 Gridworld converged in a small number of sweeps, illustrating the fast contraction property of Dynamic Programming methods.
- Off-policy Monte Carlo with Weighted Importance Sampling (Q4, Î³ = 0.9) does not require knowledge of the transition model, but relies on sampled episodes and therefore requires significantly more data and computation to approximate the optimal value function.
-Although different discount factors were used across problems (Q1: Î³ = 0.9 Q2: Î³ = 0.98, Q3: Î³ = 0.99, Q4: Î³ = 0.9), this was intentional and aligned with the experimental setup of each task. The choice of Î³ influences how strongly future rewards are weighted but does not affect the correctness of the algorithms.
- Despite differences in Î³ and computational effort, both Dynamic Programming (model-based) and Monte Carlo (model-free) approaches converged to highly similar value functions and greedy policies, confirming theoretical expectations.
- In-place and synchronous Value Iteration produced identical convergence behavior in our implementation due to sweep ordering. This demonstrates that update order can influence intermediate propagation speed, but does not change the final optimal solution.